{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# テストの実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import subprocess\n",
    "import logging\n",
    "from concurrent import futures\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TESTSET_DIR = os.path.join('/home', 'jovyan', 'work', '01_testset')\n",
    "PRJ_DIR = os.path.join('/home', 'jovyan', 'work')\n",
    "\n",
    "PROG_PATH = os.path.join(PRJ_DIR, 'main')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## マスタの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_seed_df = pd.read_csv(os.path.join(TESTSET_DIR, '01_testset_pre_master.csv'))\n",
    "sys_seed_df = pd.read_csv(os.path.join(TESTSET_DIR, '02_testset_sys_master.csv'))\n",
    "stress_seed_df = pd.read_csv(os.path.join(TESTSET_DIR, '03_testset_stress_master.csv'))\n",
    "param_seed_df = pd.read_csv(os.path.join(TESTSET_DIR, '04_testset_param_master.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 相対スコア用にChampionDataの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    CHAMP_TAG = 'yyyymmdd_hhmm'\n",
    "    CHAMP_DIR = os.path.join('/home', 'jovyan', 'work', 'result', 'champion')\n",
    "\n",
    "    champ_path = os.path.join(CHAMP_DIR, 'champ_all_{}.csv'.format(CHAMP_TAG))\n",
    "    champ_df = pd.read_csv(champ_path)\n",
    "\n",
    "    top_rate = 1.00  # 順位表を参考にチャンピオンスコアを補正\n",
    "    champ_score_dict = {}\n",
    "\n",
    "    for _, row in champ_df.iterrows():\n",
    "        seed = row['seed']\n",
    "        score = row['champion_score']\n",
    "\n",
    "        champ_score_dict[seed] = score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実行するロジックの指定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 実行プログラムにタグをつけておく\n",
    "## Champion管理用に単語は -(ハイフン) で区切る\n",
    "PROG_TAG = 'first-prog'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve(seed):\n",
    "    problem_path = os.path.join(TESTSET_DIR, 'in', '{:0>4}.txt'.format(seed)) \n",
    "    # problem_path = os.path.join(TESTSET_DIR, 'in', '{}.in'.format(seed)) \n",
    "\n",
    "    command_str = 'echo {} | {}'.format(problem_path, PROG_PATH)\n",
    "\n",
    "    # stack overflow対策\n",
    "    # command_str = 'ulimit -S -s 1048576 && echo {} | {}'.format(problem_path, PROG_PATH)\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "    res = subprocess.run(command_str, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n",
    "\n",
    "    # 経過時間(ミリ秒単位)\n",
    "    e_time = time.perf_counter() - start_time\n",
    "    e_time = int(1000 * e_time)    \n",
    "    \n",
    "    #print('{}'.format(prob_id))    \n",
    "    return (seed, e_time, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "fmt = \"%(asctime)s: %(message)s\"\n",
    "logging.basicConfig(level=logging.INFO, format=fmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(testset_name):\n",
    "    result_df = pd.DataFrame()\n",
    "    future_list = []\n",
    "\n",
    "    logger.info('Start')\n",
    "\n",
    "    testset_path = os.path.join(TESTSET_DIR, testset_name + '_master.csv')\n",
    "    testset_df = pd.read_csv(testset_path)\n",
    "\n",
    "    # 24並列実行\n",
    "    with futures.ThreadPoolExecutor(max_workers=24) as executor:\n",
    "        seed_list = testset_df['seed'].to_list()\n",
    "        future_list = list(tqdm(executor.map(solve, seed_list), total=len(seed_list)))\n",
    "\n",
    "    for future in future_list:\n",
    "        seed, e_time, res = future\n",
    "\n",
    "        # 結果をまとめる\n",
    "        solve_result = []\n",
    "        \n",
    "        solve_result.append(testset_name)\n",
    "\n",
    "        # 問題パラメタ\n",
    "        solve_result.append(seed)\n",
    "\n",
    "        # 経過時間\n",
    "        solve_result.append(e_time)\n",
    "        \n",
    "        try:\n",
    "            # -- start -- 生成コード貼り付け先\n",
    "            elem_cnt = 2\n",
    "\n",
    "            result = str(res.stderr.decode('utf-8').split()[-elem_cnt + 0].replace('Result=', ''))\n",
    "            score = int(res.stderr.decode('utf-8').split()[-elem_cnt + 1].replace('Score=', ''))\n",
    "\n",
    "            solve_result.append(score)\n",
    "            solve_result.append(result)   \n",
    "            # -- end -- 生成コード貼り付け先\n",
    "\n",
    "            # 相対スコア\n",
    "            # rel_score = int(10 ** 9 * top_rate * champ_score_dict[seed] / score)\n",
    "            # solve_result.append(rel_score)\n",
    "\n",
    "        except Exception as e:\n",
    "            print('Error: seed={}'.format(seed))\n",
    "            print(e)\n",
    "            return\n",
    "\n",
    "        result_df = pd.concat([result_df, pd.DataFrame(solve_result).T], axis=0)\n",
    "\n",
    "    logger.info('finish!')\n",
    "    \n",
    "    # 結果を整形\n",
    "    result_df.index = range(result_df.shape[0])\n",
    "    cols = ['testset', 'seed', 'time', 'score', 'result']\n",
    "    result_df.columns = cols\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary_df(result_df):\n",
    "    # 全体サマリ\n",
    "    summary_all_df = pd.DataFrame()\n",
    "\n",
    "    for testset in np.unique(result_df['testset']):\n",
    "        test_result_df = result_df.query('testset == \"{}\"'.format(testset))\n",
    "\n",
    "        summary_df = pd.DataFrame(\n",
    "        {\n",
    "            'testset': [testset],\n",
    "            'tag': [PROG_TAG],\n",
    "            \n",
    "            'time_mean': [int(np.mean(test_result_df['time']))],\n",
    "            \n",
    "            # -- start -- 生成コード貼り付け先\n",
    "            'score_mean': [np.mean(test_result_df['score'])],\n",
    "            'score_min': [min(test_result_df['score'])],\n",
    "            'score_max': [max(test_result_df['score'])],\n",
    "            # -- end -- 生成コード貼り付け先\n",
    "\n",
    "            'time_max': [max(test_result_df['time'])],\n",
    "        })\n",
    "\n",
    "        summary_all_df = pd.concat([summary_all_df, summary_df], axis=0)   \n",
    "\n",
    "    return summary_all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! cd .. && make clean > /dev/null && make -j CFLAGS_EXTRA=\"-DLOCAL_JUDGE\" > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROG_NAME_LIST = ['main']\n",
    "#PROG_NAME_LIST = ['main', 'main_off']\n",
    "\n",
    "testset_name = '01_testset_pre'\n",
    "#testset_name = '02_testset_sys'\n",
    "#testset_name = '03_testset_stress'\n",
    "#testset_name = '04_testset_param'\n",
    "\n",
    "result_dict = {}\n",
    "summary_all_dict = {}\n",
    "\n",
    "for PROG_NAME in PROG_NAME_LIST:\n",
    "    prog_path = os.path.join(PRJ_DIR, PROG_NAME)\n",
    "    \n",
    "    result_df = pd.DataFrame()\n",
    "    \n",
    "    testset_result_df = run_test(testset_name)\n",
    "    result_df = pd.concat([result_df, testset_result_df], axis=0)\n",
    "    \n",
    "    result_dict[PROG_NAME] = result_df\n",
    "    summary_all_dict[PROG_NAME] = get_summary_df(result_df)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre test\n",
    "result_sub_df = pd.merge(pre_seed_df, result_df, on='seed')\n",
    "summary_df = get_summary_df(result_sub_df)\n",
    "summary_df['testset'] = '01_test_pre'\n",
    "\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys param\n",
    "if testset_name == '04_testset_param' or testset_name == '02_testset_sys' or testset_name == '03_testset_stress':\n",
    "    summary_df = pd.DataFrame()\n",
    "    \n",
    "    result_sub_df = pd.merge(param_seed_df, result_df, on='seed')\n",
    "    summary_df = get_summary_df(result_sub_df)\n",
    "    summary_df['testset'] = '04_test_param'\n",
    "\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys test\n",
    "if testset_name == '02_testset_sys' or testset_name == '03_testset_stress':\n",
    "    summary_df = pd.DataFrame()\n",
    "    \n",
    "    result_sub_df = pd.merge(sys_seed_df, result_df, on='seed')\n",
    "    summary_df = get_summary_df(result_sub_df)\n",
    "    summary_df['testset'] = '02_test_sys'\n",
    "\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stress test\n",
    "if testset_name == '03_testset_stress':\n",
    "    summary_df = pd.DataFrame()\n",
    "    \n",
    "    result_sub_df = pd.merge(stress_seed_df, result_df, on='seed')\n",
    "    summary_df = get_summary_df(result_sub_df)\n",
    "    summary_df['testset'] = '03_test_stress'\n",
    "\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_df.sort_values('time', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#result_df.sort_values('rel_score', ascending=True).head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 結果ログの保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_now = datetime.datetime.now() + datetime.timedelta(hours=9)\n",
    "time_str = t_now.strftime('%Y%m%d_%H%M')\n",
    "\n",
    "for PROG_NAME in PROG_NAME_LIST:\n",
    "    result_df = result_dict[PROG_NAME]\n",
    "    \n",
    "    for testset in np.unique(result_df['testset']):\n",
    "        csv_df = result_df.query('testset == \"{}\"'.format(testset))\n",
    "        csv_df.to_csv(PRJ_DIR+'/result/{}_{}_{}_{}.csv'.format(time_str,PROG_TAG, testset, PROG_NAME), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp $PRJ_DIR/main $PRJ_DIR/result/bin/$PROG_TAG"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
